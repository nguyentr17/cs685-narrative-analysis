{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import little_mallet_wrapper as lmw\n",
    "import pandas as pd \n",
    "import ast \n",
    "from lmw import *\n",
    "import textwrap\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "MALLET_PATH = \"~/mallet/bin/mallet\"\n",
    "NAR_POS_PATH = \"../../narrative_detection/narrative_posts_by_trained_classification.csv\"\n",
    "num_top = [10, 15, 20, 30]\n",
    "output_directory_path = \"../data/output/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with at least 2 narrative positive posts: 901\n",
      "Average number of words per post: 220.56056338028168\n",
      "Standard deviation of number of words per post: 141.52999182363467\n",
      "Min/max number of words per post: 100 1913\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>subreddit_id</th>\n",
       "      <th>selftext</th>\n",
       "      <th>title</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-CreamyPie-</td>\n",
       "      <td>t5_o3plh</td>\n",
       "      <td>[Hello!  Im 11 and Im a boy self recovering fr...</td>\n",
       "      <td>[Fear Food Friday!! So far its successful, Fea...</td>\n",
       "      <td>[gbj0rh, gagddu, g7xk05]</td>\n",
       "      <td>[1588343070, 1588190127, 1587836033]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>194569324</td>\n",
       "      <td>t5_s5o7i</td>\n",
       "      <td>[Just want to get this off my chest because I ...</td>\n",
       "      <td>[Hardcore restricting, close to being discharg...</td>\n",
       "      <td>[g3255g, fvycov]</td>\n",
       "      <td>[1587130632, 1586178189]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>197326743251b</td>\n",
       "      <td>t5_2tmc8</td>\n",
       "      <td>[i can eat without rules   im ok with not purg...</td>\n",
       "      <td>[body image is the last thing to go, sensory o...</td>\n",
       "      <td>[pn4k5v, pjyyfb, oyf5pl]</td>\n",
       "      <td>[1631492734, 1631058338, 1628161076]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40sareinteresting</td>\n",
       "      <td>t5_2tmc8</td>\n",
       "      <td>[I have been off and on bp for 20 years. It’s ...</td>\n",
       "      <td>[Binging and gaining weight fast, How to truly...</td>\n",
       "      <td>[zyx8lq, ywoeic]</td>\n",
       "      <td>[1672395862, 1668588759]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50gayrats</td>\n",
       "      <td>t5_rbmui</td>\n",
       "      <td>[Today my dad called go make sure I ate I said...</td>\n",
       "      <td>[Eating disorder Health scare, My brother is s...</td>\n",
       "      <td>[10900qu, zy1gd0]</td>\n",
       "      <td>[1673426889, 1672307211]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              author subreddit_id  \\\n",
       "0        -CreamyPie-     t5_o3plh   \n",
       "1          194569324     t5_s5o7i   \n",
       "2      197326743251b     t5_2tmc8   \n",
       "3  40sareinteresting     t5_2tmc8   \n",
       "4          50gayrats     t5_rbmui   \n",
       "\n",
       "                                            selftext  \\\n",
       "0  [Hello!  Im 11 and Im a boy self recovering fr...   \n",
       "1  [Just want to get this off my chest because I ...   \n",
       "2  [i can eat without rules   im ok with not purg...   \n",
       "3  [I have been off and on bp for 20 years. It’s ...   \n",
       "4  [Today my dad called go make sure I ate I said...   \n",
       "\n",
       "                                               title  \\\n",
       "0  [Fear Food Friday!! So far its successful, Fea...   \n",
       "1  [Hardcore restricting, close to being discharg...   \n",
       "2  [body image is the last thing to go, sensory o...   \n",
       "3  [Binging and gaining weight fast, How to truly...   \n",
       "4  [Eating disorder Health scare, My brother is s...   \n",
       "\n",
       "                         id                           created_utc  \n",
       "0  [gbj0rh, gagddu, g7xk05]  [1588343070, 1588190127, 1587836033]  \n",
       "1          [g3255g, fvycov]              [1587130632, 1586178189]  \n",
       "2  [pn4k5v, pjyyfb, oyf5pl]  [1631492734, 1631058338, 1628161076]  \n",
       "3          [zyx8lq, ywoeic]              [1672395862, 1668588759]  \n",
       "4         [10900qu, zy1gd0]              [1673426889, 1672307211]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive narrative dataset grouped by user ----\n",
    "positive_df = pd.read_csv(NAR_POS_PATH)[['subreddit_id', 'author', 'selftext', 'title', 'id', 'created_utc']]\n",
    "positive_df['selftext'] = positive_df['selftext'].replace(r'\\n',' ', regex=True) \n",
    "positive_users = positive_df.groupby([\"author\", \"subreddit_id\"]).agg(list).reset_index()\n",
    "positive_users = positive_users[positive_users['created_utc'].apply(lambda x: len(x) >= 2 and len(x) < 50)].reset_index(drop=True)\n",
    "\n",
    "sum_post = []\n",
    "for cluster in positive_users['selftext']: \n",
    "    for post in cluster: \n",
    "        sum_post.append(len(post.split()))\n",
    "print(\"Number of users with at least 2 narrative positive posts:\", len(positive_users))\n",
    "print(\"Average number of words per post:\", sum(sum_post)/len(sum_post))\n",
    "print(\"Standard deviation of number of words per post:\", np.std(sum_post))\n",
    "print(\"Min/max number of words per post:\", min(sum_post), max(sum_post))\n",
    "positive_users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating stopwords list ----\n",
    "# Custom stopwords\n",
    "with open(\"../data/input/custom_stop.txt\", \"r\") as f:\n",
    "    custom_stop = f.read().split()\n",
    "\n",
    "# TF-IDF stopwords\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(positive_df['selftext'].tolist())\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "doc_frequency = (df_tfidf != 0).sum(axis=0)\n",
    "doc_frequency = doc_frequency / len(positive_df)\n",
    "df_tfidf = df_tfidf.loc[:, doc_frequency >= 0.5]\n",
    "stop_tfidf = df_tfidf.columns.tolist()\n",
    "\n",
    "# Final stoplist\n",
    "stoplist = stopwords.words('english') + [\"amp\", \"like\"] + stop_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training data ----\n",
    "training_data = [lmw.process_string(t) for post in positive_users['selftext'] for t in post]\n",
    "training_data = [d for d in training_data if d.strip()]\n",
    "\n",
    "# Generate groupings ----\n",
    "groupings = []\n",
    "counting = 0 \n",
    "for item in positive_users['selftext']: \n",
    "    idx = []\n",
    "    for post in item: \n",
    "        idx.append(counting)\n",
    "        counting += 1\n",
    "    groupings.append(idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Topic Model\n",
    "- Training data:\n",
    "    + All posts in the database where the users post at least twice and fewer than 50 times.\n",
    "    + Each document represent a post (post-level topic modeling).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor num in num_top: \\n    lmw_training(num, output_directory_path, training_data)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training ----\n",
    "'''\n",
    "for num in num_top: \n",
    "    lmw_training(num, output_directory_path, training_data)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwith open(output_directory_path + \"/top_view.txt\", \\'w\\') as f: \\n    for num in num_top: \\n        topic_keys = lmw.load_topic_keys(output_directory_path + \"/mallet.topic_keys.\" + str(num))\\n        f.write(\"Topics for k=\" + str(num) + \"\\n\")\\n        for i, t in enumerate(topic_keys):\\n            line = str(i) + \\'\\t\\' + \\' \\'.join(t[:10]) + \"\\n\"\\n            f.write(line)\\n        f.write(\\'\\n\\')\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examining all topics (top_view.txt) ----\n",
    "'''\n",
    "with open(output_directory_path + \"/top_view.txt\", 'w') as f: \n",
    "    for num in num_top: \n",
    "        topic_keys = lmw.load_topic_keys(output_directory_path + \"/mallet.topic_keys.\" + str(num))\n",
    "        f.write(\"Topics for k=\" + str(num) + \"\\n\")\n",
    "        for i, t in enumerate(topic_keys):\n",
    "            line = str(i) + '\\t' + ' '.join(t[:10]) + \"\\n\"\n",
    "            f.write(line)\n",
    "        f.write('\\n')\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Topics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Topic Transition\n",
    "Following Akoury 2020, we examine common local topic transitions between entries written by the same user across time. We compute the transition probability from topic A to topic B by counting how many times A and B are the most probable topics for two consecutive entries, respectively, and normalizing by the total number of occurrences of topic A.\n",
    "\n",
    "- Table in the paper: Topics with the highest relative importance, which illustrate the diversity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['purging binge purge binging stop bulimia b/p day cycle bad',\n",
       " 'xNUMb hair would teeth dentist make mouth abs use email',\n",
       " 'mom family fat dad sister said parents told comments always',\n",
       " 'like feel even want know much get hate fucking never',\n",
       " 'work get want time going need life back job hard',\n",
       " 'like feel really also know lot think still idk maybe',\n",
       " 'food mad buy money cookies store candy buying kitchen fucking',\n",
       " 'know really get want eating think help tell disorder also',\n",
       " 'eat eating food feel like meal even day hungry want',\n",
       " 'people post group recovery made diet disordered certain looking posts',\n",
       " 'weight gain lose gained eating back underweight bmi want NUM',\n",
       " 'body look weight see clothes skinny fat looked face fit',\n",
       " 'foods ate NUM eat food sugar chocolate cream ice cheese',\n",
       " 'life eating self disorder things every people anxiety way mental',\n",
       " 'never heart pain sick enough blood sleep night take water',\n",
       " 'today didn felt got day going last back went time',\n",
       " 'treatment inpatient hospital therapist NUM program get appointment doctor medical',\n",
       " 'recovery body anyone really still much long time recover would',\n",
       " 'NUM years since time year last months started ago even',\n",
       " 'NUM day calories eat days also exercise every gym week']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 20             # Change this to view different output files \n",
    "\n",
    "# Viewing topics ---- \n",
    "topic_keys = lmw.load_topic_keys(output_directory_path + \"/mallet.topic_keys.\" + str(num_topics))\n",
    "topic_label = []\n",
    "for i, t in enumerate(topic_keys):\n",
    "    topic_label.append(' '.join(t[:10]))\n",
    "topic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lmw.load_topic_distributions(output_directory_path + \"/mallet.topic_distributions.\" + str(num_topics))\n",
    "\n",
    "# Get the most probable topic and topic distribution for each document ---- \n",
    "most_prob, topics = [], []\n",
    "for doc in range(len(topic_distributions)): \n",
    "    item = topic_distributions[doc]\n",
    "    most_prob.append(item.index(max(item)))\n",
    "    topics.append(item)\n",
    "\n",
    "# Get the most probable topic and topic distribution for each document (grouped by user) ----\n",
    "most_prob_grouped, prob_grouped = [], []\n",
    "for group in groupings:\n",
    "    most_prob_grouped.append([most_prob[i] for i in group])\n",
    "    prob_grouped.append([topics[i] for i in group])\n",
    "\n",
    "positive_users['Topic Distribution'] = prob_grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most probable topic and topic distribution for each document (ungrouped) ----\n",
    "exploded = positive_users.explode(['selftext', 'created_utc', 'title', 'Topic Distribution', 'id']).to_csv(\"../data/positive_topic_dist.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local topic transition between entries within the same user ----\n",
    "# For each user, get the transition probability from topic A to topic B by\n",
    "# counting the number of times topic B appears after another topic A\n",
    "# normalize by the total number of times topic A is the most probable topic\n",
    "\n",
    "# Get the transition probability for each user ----\n",
    "# transition_prob: List of dictionaries {topic A: {topic B: countBA}}\n",
    "# topic_count: List of dictionaries {topic A: countA}\n",
    "transition_count, topic_count = [], []\n",
    "for user in most_prob_grouped:\n",
    "    transition, topic = {}, {}\n",
    "    for i in range(len(user)-1):\n",
    "        if user[i] not in transition:\n",
    "            transition[user[i]] = {user[i+1]: 1}\n",
    "        else: \n",
    "            transition[user[i]][user[i+1]] = transition[user[i]].get(user[i+1], 0) + 1\n",
    "        topic[user[i]] = topic.get(user[i], 0) + 1\n",
    "    transition_count.append(transition)\n",
    "    topic_count.append(topic)\n",
    "\n",
    "# Normalize the transition probability ----\n",
    "transition_prob_norm = []\n",
    "for user in range(len(transition_count)): \n",
    "    transition_norm = {}\n",
    "    for topicA in transition_count[user]: \n",
    "        for topicB in transition_count[user][topicA]: \n",
    "            transition_norm[topicA] = transition_norm.get(topicA, {})\n",
    "            transition_norm[topicA][topicB] = transition_count[user][topicA][topicB] / topic_count[user][topicA]\n",
    "    transition_prob_norm.append(transition_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the most probable transition for each user----\n",
    "max_top = []\n",
    "for user in transition_prob_norm:\n",
    "    maxing = {}\n",
    "    for topic in user:\n",
    "        max_val = 0 \n",
    "        max_topic = []\n",
    "        for t in user[topic]:\n",
    "            if user[topic][t] > max_val: \n",
    "                max_val = user[topic][t]\n",
    "                max_topic = [t]\n",
    "            elif user[topic][t] == max_val: \n",
    "                max_topic.append(t)\n",
    "        maxing[topic] = max_topic\n",
    "    max_top.append(maxing)\n",
    "\n",
    "\n",
    "# Reformat max_top into a list of lists of tuples ----\n",
    "all_users = []\n",
    "for user in max_top: \n",
    "    user_list = []\n",
    "    for topic in user: \n",
    "        for t in user[topic]: \n",
    "            user_list.append((topic, t))\n",
    "    all_users.append(user_list)\n",
    "all_users_expanded = [user[i] for user in all_users for i in range(len(user))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timeShort (moderate) ----> feeling (moderate)\n",
      "formalTreatment (good) ----> feeling (moderate)\n",
      "feeling (moderate) ----> feeling (moderate)\n",
      "timeLong (moderate) ----> feeling (moderate)\n",
      "eat(good) ----> eat(good)\n",
      "recovery (bad) ----> recovery (bad)\n",
      "weight (good) ----> weight (good)\n",
      "feeling (moderate) ----> feeling (moderate)\n",
      "community (moderate) ----> feeling (moderate)\n",
      "food (good) ----> feeling (moderate)\n",
      "work (bad) ----> work (bad)\n",
      "feeling (moderate) ----> feeling (moderate)\n",
      "calories (bad) ----> feeling (moderate)\n",
      "bodyParts (good) ----> weight (good)\n",
      "family (good) ----> feeling (moderate)\n",
      "eating ----> feeling (moderate)\n",
      "appearance (good) ----> feeling (moderate)\n",
      "groceryShopping (moderate) ----> eat(good)\n",
      "life (bad) ----> life (bad)\n",
      "discomfort (moderate) ----> timeShort (moderate)\n"
     ]
    }
   ],
   "source": [
    "# Get the most frequent transition across all users ----\n",
    "max_top_freq = {}\n",
    "for tup in all_users_expanded:\n",
    "    # get the most frequent transition for each user \n",
    "    if tup[0] not in max_top_freq: \n",
    "        max_top_freq[tup[0]] = {tup[1]: 1}\n",
    "    else:\n",
    "        max_top_freq[tup[0]][tup[1]] = max_top_freq[tup[0]].get(tup[1], 0) + 1\n",
    "\n",
    "# Get the value with the highest count for each topic ----\n",
    "# Topic     Most Frequent Transition\n",
    "topic_label = {}\n",
    "with open(\"../data/analysis/topic_label_20.txt\", 'r') as f:\n",
    "    labs = f.readlines()\n",
    "    labs = [t.strip() for t in labs]\n",
    "    for topic in labs: \n",
    "        idx, label, keywords = topic.split(\"_\")[0],  topic.split(\"_\")[1],  topic.split(\"_\")[2]\n",
    "        if str(idx) not in topic_label: \n",
    "            topic_label[str(idx)] = [label, keywords]\n",
    "\n",
    "print('Most common local topic transitions across users:')\n",
    "for topic in max_top_freq: \n",
    "    print(topic_label[str(topic)][0], \"---->\", topic_label[str(max(max_top_freq[topic], key=max_top_freq[topic].get))][0])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic-sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Antoniak (2019)'s time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, get the ranking of all the documents\n",
    "ranking_doc = {}\n",
    "for i in range(num_topics):\n",
    "    ranking_doc[topic_label[i]] = []\n",
    "    for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=i, n=10):\n",
    "        ranking_doc[topic_label[i]].append(str(training_data.index(d)))\n",
    "\n",
    "# Get the position of each document in the ranking\n",
    "ranking_pos = {}\n",
    "for topic in ranking_doc: \n",
    "    for doc in ranking_doc[topic]: \n",
    "        if doc not in ranking_pos:\n",
    "            ranking_pos[doc] = [(topic, ranking_doc[topic].index(doc))]\n",
    "        else: \n",
    "            ranking_pos[doc].append((topic, ranking_doc[topic].index(doc)))\n",
    "\n",
    "ranking_pos = {k: sorted(v, key=lambda x: x[1]) for k, v in ranking_pos.items()}\n",
    "ranking_pos = dict(sorted(ranking_pos.items(), key=lambda item: int(item[0])))\n",
    "\n",
    "# For each topic, get its probability of appearing in each document \n",
    "ranking_topic_prob = {}\n",
    "for i in range(num_topics):\n",
    "    ranking_topic_prob[topic_label[i]] = []\n",
    "    for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=i, n=len(training_data)):\n",
    "        ranking_topic_prob[topic_label[i]].append((str(training_data.index(d)), p))\n",
    "# Sort ranking_topic_prob by the first index of the value tuple\n",
    "ranking_topic_prob = {k: sorted(v, key=lambda x: int(x[0])) for k, v in ranking_topic_prob.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
