{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import little_mallet_wrapper as lmw\n",
    "import pandas as pd \n",
    "import ast \n",
    "from lmw import *\n",
    "\n",
    "MALLET_PATH = \"~/mallet/bin/mallet\"\n",
    "POSITIVE_USER_PATH = \"../data/positive_user.csv\"\n",
    "POSITIVE_NAR_PATH = \"../data/narrative_positive.csv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_users = pd.read_csv(POSITIVE_USER_PATH)\n",
    "positive_users['selftext'] = positive_users['selftext'].apply(ast.literal_eval)\n",
    "positive_users['created_utc'] = positive_users['created_utc'].apply(ast.literal_eval)\n",
    "positive_users['link_flair_text'] = positive_users['link_flair_text'].apply(ast.literal_eval)\n",
    "positive_users['title'] = positive_users['title'].apply(ast.literal_eval)\n",
    "positive_users.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multiple_df = positive_users[positive_users['selftext'].apply(lambda x: len(x) > 1)].reset_index(drop=True)\n",
    "multiple_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = pd.read_csv(POSITIVE_NAR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = []\n",
    "for posts in positive_users['selftext']:\n",
    "    for post in posts: \n",
    "        word_count.append(len(post.split(\".\")))\n",
    "word_count = pd.Series(word_count)\n",
    "word_count.hist(bins=50, range=[0,500])\n",
    "word_count.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling (temporal)\n",
    "- Training data:\n",
    "    + Selected narrative-positive posts from users who post at least twice. \n",
    "    + Each document represents a post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [sub for i in multiple_df['selftext'] for sub in i]\n",
    "training_data = [lmw.process_string(t) for t in text]\n",
    "training_data = [d for d in training_data if d.strip()]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 5\n",
    "output_directory_path = \"../data/output/temporal-output/\"\n",
    "lmw_training(num_topics, output_directory_path, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys = lmw.load_topic_keys(output_directory_path + \"mallet.topic_keys.\" + str(num_topics))\n",
    "topic_label = []\n",
    "for i, t in enumerate(topic_keys):\n",
    "    print(i, '\\t', ' '.join(t[:10]))\n",
    "    topic_label.append(t[0])\n",
    "topic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, get the ranking of all the documents\n",
    "ranking_doc = {}\n",
    "topic_distributions = lmw.load_topic_distributions(output_directory_path + \"mallet.topic_distributions.\" + str(num_topics))\n",
    "for i in range(num_topics):\n",
    "    ranking_doc[topic_label[i]] = []\n",
    "    for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=i, n=15):\n",
    "        if p > 0.1: \n",
    "            ranking_doc[topic_label[i]].append(str(training_data.index(d)))\n",
    "ranking_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the position of each document in the ranking\n",
    "ranking_pos = {}\n",
    "for topic in ranking_doc: \n",
    "    for doc in ranking_doc[topic]: \n",
    "        if doc not in ranking_pos:\n",
    "            ranking_pos[doc] = [(topic, ranking_doc[topic].index(doc))]\n",
    "        else: \n",
    "            ranking_pos[doc].append((topic, ranking_doc[topic].index(doc)))\n",
    "\n",
    "ranking_pos = {k: sorted(v, key=lambda x: x[1]) for k, v in ranking_pos.items()}\n",
    "ranking_pos = dict(sorted(ranking_pos.items(), key=lambda item: int(item[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (0,1), (2,3), (4,5,6), (7,8), (9,10), (11,12), (13,14) belong to the same user\n",
    "# (7,8) is about filiming documentary\n",
    "for item in ranking_pos: \n",
    "    print(item, ranking_pos[item][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really interesting result because most users post about the same topic over time. Can be because the number of topics is small. However, k>5 returns topics of lower quality. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling (General)\n",
    "- Training data:\n",
    "    + Selected narrative-positive posts. \n",
    "    + Each document represent a post. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [lmw.process_string(t) for t in positive_df['selftext'].tolist()]\n",
    "training_data = [d for d in training_data if d.strip()]\n",
    "training_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 15\n",
    "output_directory_path = \"../data/output/general-output/\"\n",
    "lmw_training(num_topics, output_directory_path, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keys = lmw.load_topic_keys(output_directory_path + \"mallet.topic_keys.\" + str(num_topics))\n",
    "topic_label = []\n",
    "for i, t in enumerate(topic_keys):\n",
    "    print(i, '\\t', ' '.join(t[:10]))\n",
    "    topic_label.append(t[0])\n",
    "topic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each topic, get the ranking of all the documents\n",
    "ranking_doc = {}\n",
    "topic_distributions = lmw.load_topic_distributions(output_directory_path + \"mallet.topic_distributions.\" + str(num_topics))\n",
    "for i in range(num_topics):\n",
    "    ranking_doc[topic_label[i]] = []\n",
    "    for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=i, n=15):\n",
    "        if p > 0.1: \n",
    "            ranking_doc[topic_label[i]].append(str(training_data.index(d)))\n",
    "ranking_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the position of each document in the ranking\n",
    "ranking_pos = {}\n",
    "for topic in ranking_doc: \n",
    "    for doc in ranking_doc[topic]: \n",
    "        if doc not in ranking_pos:\n",
    "            ranking_pos[doc] = [(topic, ranking_doc[topic].index(doc))]\n",
    "        else: \n",
    "            ranking_pos[doc].append((topic, ranking_doc[topic].index(doc)))\n",
    "\n",
    "ranking_pos = {k: sorted(v, key=lambda x: x[1]) for k, v in ranking_pos.items()}\n",
    "ranking_pos = dict(sorted(ranking_pos.items(), key=lambda item: int(item[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top topic for each document\n",
    "for item in ranking_pos: \n",
    "    print(item, ranking_pos[item][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
