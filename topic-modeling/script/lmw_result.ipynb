{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import little_mallet_wrapper as lmw\n",
    "import pandas as pd \n",
    "import ast \n",
    "from lmw import *\n",
    "import textwrap\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "STOP = stopwords.words('english')\n",
    "STOP.append(\"amp\")\n",
    "\n",
    "MALLET_PATH = \"~/mallet/bin/mallet\"\n",
    "USER_POS_PATH = \"../data/user_positive.csv\"\n",
    "USER_SQL_PATH = \"../data/user_sqlite.csv\"\n",
    "NAR_POS_PATH = \"../../narrative_detection/narrative_posts_by_trained_classification.csv\"\n",
    "num_top = [5,10,15,20]\n",
    "output_directory_path = \"../data/output/pos-output/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users with a narrative positive post: 5161\n",
      "Number of users with at least 2 narrative positive posts: 901\n"
     ]
    }
   ],
   "source": [
    "# Positive narrative dataset grouped by user ----\n",
    "positive_nar = pd.read_csv(NAR_POS_PATH)\n",
    "positive_users = pd.read_csv(USER_POS_PATH)\n",
    "positive_users['selftext'] = positive_users['selftext'].apply(ast.literal_eval)\n",
    "positive_users['merged'] = positive_users['selftext'].apply(lambda x: ' '.join(x))\n",
    "positive_users['created_utc'] = positive_users['created_utc'].apply(ast.literal_eval)\n",
    "print(\"Number of users with a narrative positive post:\", len(positive_users))\n",
    "positive_users = positive_users[positive_users['created_utc'].apply(lambda x: len(x) >= 2 and len(x) < 50)].reset_index(drop=True)\n",
    "print(\"Number of users with at least 2 narrative positive posts:\", len(positive_users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training data ----\n",
    "training_data = [lmw.process_string(t) for post in positive_users['selftext'] for t in post]\n",
    "training_data = [d for d in training_data if d.strip()]\n",
    "\n",
    "# Generate groupings ----\n",
    "groupings = []\n",
    "counting = 0 \n",
    "for item in positive_users['selftext']: \n",
    "    idx = []\n",
    "    for post in item: \n",
    "        idx.append(counting)\n",
    "        counting += 1\n",
    "    groupings.append(idx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling\n",
    "- Training data:\n",
    "    + All posts in the database where the users post at least twice and fewer than 50 times.\n",
    "    + Each document represent a post. \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "- Do not run if only doing analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training ----\n",
    "for num in num_top: \n",
    "    lmw_training(num, output_directory_path, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining all topics (top_view.txt) ----\n",
    "with open(output_directory_path + \"top_view.txt\", 'w') as f: \n",
    "    for num in num_top: \n",
    "        topic_keys = lmw.load_topic_keys(output_directory_path + \"mallet.topic_keys.\" + str(num))\n",
    "        f.write(\"Topics for k=\" + str(num) + \"\\n\")\n",
    "        for i, t in enumerate(topic_keys):\n",
    "            line = str(i) + '\\t' + ' '.join(t[:10]) + \"\\n\"\n",
    "            f.write(line)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Analysis\n",
    "\n",
    "Following Akoury 2020, we examine common local topic transitions between entries written by the same user across time. We compute the transition probability from topic A to topic B by counting how many times A and B are the most probable topics for two consecutive entries, respectively, and normalizing by the total number of occurrences of topic A.\n",
    "\n",
    "- Table in the paper: Topics with the highest relative importance, which illustrate the diversity of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doctor take heart NUM blood taking said laxatives hospital drink',\n",
       " 'recovery body still restriction hair physical long extreme able anyone',\n",
       " 'food NUM eat ate foods one sugar chocolate cream today',\n",
       " 'know like want really feel eating get think even also',\n",
       " 'life never would people enough love every xNUMb body world',\n",
       " 'said told people like one didn mom look always never',\n",
       " 'eat NUM eating day food calories days binge hungry meal',\n",
       " 'like feel want even know going time fucking get day',\n",
       " 'NUM work last treatment time get year back years got',\n",
       " 'weight NUM body gain lose lost look back gained fat']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_topics = 10             # Change this to view different output files \n",
    "\n",
    "# Viewing topics ---- \n",
    "topic_keys = lmw.load_topic_keys(output_directory_path + \"mallet.topic_keys.\" + str(num_topics))\n",
    "topic_label = []\n",
    "for i, t in enumerate(topic_keys):\n",
    "    topic_label.append(' '.join(t[:10]))\n",
    "topic_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_distributions = lmw.load_topic_distributions(output_directory_path + \"mallet.topic_distributions.\" + str(num_topics))\n",
    "\n",
    "# Get the most probable topic for each document ---- \n",
    "most_prob = []\n",
    "for doc in range(len(topic_distributions)): \n",
    "    item = topic_distributions[doc]\n",
    "    most_prob.append(item.index(max(item)))\n",
    "\n",
    "# Get the most probable topic for each document (grouped by user) ----\n",
    "most_prob_grouped = []\n",
    "for group in groupings:\n",
    "    most_prob_grouped.append([most_prob[i] for i in group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most probable topic transition across all users: {2: 3, 3: 3, 9: 3, 6: 3, 7: 3, 1: 1, 4: 3, 8: 3, 0: 3, 5: 3}\n"
     ]
    }
   ],
   "source": [
    "# Local topic transition between entries within the same user ----\n",
    "# For each user, get the transition probability from one topic to another by\n",
    "# counting the number of times a topic appears after another topic\n",
    "# normalize by the total number of occurrences of the first topic\n",
    "\n",
    "# Get the transition probability for each user ----\n",
    "transition_prob = []\n",
    "for group in most_prob_grouped:\n",
    "    transition = {}\n",
    "    for i in range(len(group)-1):\n",
    "        if group[i] not in transition:\n",
    "            transition[group[i]] = {group[i+1]: 1}\n",
    "        else: \n",
    "            transition[group[i]][group[i+1]] = transition[group[i]].get(group[i+1], 0) + 1\n",
    "    transition_prob.append(transition)\n",
    "\n",
    "# Counting the number of occurrence for each topic ----\n",
    "topic_count = {}\n",
    "for user in most_prob_grouped:\n",
    "    for topic in user:\n",
    "        topic_count[topic] = topic_count.get(topic, 0) + 1\n",
    "\n",
    "# Normalize the transition probability ----\n",
    "transition_prob_norm = []\n",
    "for user in transition_prob:\n",
    "    user_norm = {}\n",
    "    for topic in user:\n",
    "        user_norm[topic] = {k: v/topic_count[topic] for k, v in user[topic].items()}\n",
    "    transition_prob_norm.append(user_norm)\n",
    "transition_prob_norm\n",
    "\n",
    "# Get the most probable transition for each user----\n",
    "max_top = {}\n",
    "for user in transition_prob_norm:\n",
    "    for topic in user:\n",
    "        if topic not in max_top: \n",
    "            max_top[topic] = [max(user[topic], key=user[topic].get)]\n",
    "        else: \n",
    "            max_top[topic].append(max(user[topic], key=user[topic].get))\n",
    "\n",
    "# Get the most frequent transition across all users ----\n",
    "max_top_freq = {}\n",
    "for topic in max_top:\n",
    "    max_top_freq[topic] = max(set(max_top[topic]), key=max_top[topic].count)\n",
    "\n",
    "print(\"Most probable topic transition across all users:\", max_top_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Antoniak (2019)'s time series analysis ----\n",
    "\n",
    "# For each topic, get the ranking of all the documents\n",
    "ranking_doc = {}\n",
    "for i in range(num_topics):\n",
    "    ranking_doc[topic_label[i]] = []\n",
    "    for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=i, n=10):\n",
    "        ranking_doc[topic_label[i]].append(str(training_data.index(d)))\n",
    "\n",
    "# Get the position of each document in the ranking\n",
    "ranking_pos = {}\n",
    "for topic in ranking_doc: \n",
    "    for doc in ranking_doc[topic]: \n",
    "        if doc not in ranking_pos:\n",
    "            ranking_pos[doc] = [(topic, ranking_doc[topic].index(doc))]\n",
    "        else: \n",
    "            ranking_pos[doc].append((topic, ranking_doc[topic].index(doc)))\n",
    "\n",
    "ranking_pos = {k: sorted(v, key=lambda x: x[1]) for k, v in ranking_pos.items()}\n",
    "ranking_pos = dict(sorted(ranking_pos.items(), key=lambda item: int(item[0])))\n",
    "\n",
    "# For each topic, get its probability of appearing in each document \n",
    "ranking_topic_prob = {}\n",
    "for i in range(num_topics):\n",
    "    ranking_topic_prob[topic_label[i]] = []\n",
    "    for p, d in lmw.get_top_docs(training_data, topic_distributions, topic_index=i, n=len(training_data)):\n",
    "        ranking_topic_prob[topic_label[i]].append((str(training_data.index(d)), p))\n",
    "# Sort ranking_topic_prob by the first index of the value tuple\n",
    "ranking_topic_prob = {k: sorted(v, key=lambda x: int(x[0])) for k, v in ranking_topic_prob.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs685",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
