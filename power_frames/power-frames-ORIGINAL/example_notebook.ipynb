{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import power_frames as pf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Load an example dataset\n",
    "\n",
    "We'll use the annotated subset of a Reddit personal stories corpus curated by Jessica Ouyang.\n",
    "\n",
    "You can learn about this dataset and download it here: http://www.cs.columbia.edu/~ouyangj/reddit-data/\n",
    "\n",
    "Direct download link: http://www.cs.columbia.edu/~ouyangj/reddit-data/reddit-data-annotated.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_directory_path = '/Users/maria/Documents/data/reddit-stories-ouyang/story'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stories = []\n",
    "\n",
    "for _file_name in os.listdir(dataset_directory_path):\n",
    "    stories.append(' '.join([_line.strip() for _line in open(dataset_directory_path + '/' + _file_name, 'r')]))\n",
    "\n",
    "len(stories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "story_ids = [str(i) for i in range(len(stories))]\n",
    "len(story_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Define the personas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_pattern_dict = {'masculine pronouns': r'he|him',\n",
    "                        'feminine pronouns': r'she|her',\n",
    "                        'neutral pronouns': r'they|them'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Load the power dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_path = '/Users/maria/Documents/data/FramesAgencyPower/agency_power.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2115"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_power_dict = pf.get_verb_power_dict(lexicon_path)\n",
    "len(verb_power_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "power_equal \t guess\n",
      "power_agent \t free\n",
      "power_agent \t insure\n",
      "power_agent \t pour\n",
      "power_theme \t detect\n",
      "power_agent \t warm\n",
      "power_equal \t endanger\n",
      "nan \t rocket\n",
      "nan \t curve\n",
      "power_agent \t compos\n",
      "power_agent \t spread\n",
      "power_agent \t classify\n",
      "power_agent \t reverse\n",
      "power_theme \t object\n",
      "power_agent \t invest\n",
      "power_agent \t presume\n",
      "power_agent \t show\n",
      "power_equal \t pet\n",
      "nan \t screech\n",
      "nan \t swoop\n"
     ]
    }
   ],
   "source": [
    "for _verb, _power in random.sample(verb_power_dict.items(), 20):\n",
    "    print(_power, '\\t', _verb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Measure power across the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-10-22 13:11:48 Processed 0 out of 476\n",
      "2022-10-22 13:11:52 Processed 100 out of 476\n",
      "2022-10-22 13:11:57 Processed 200 out of 476\n",
      "2022-10-22 13:12:01 Processed 300 out of 476\n",
      "2022-10-22 13:12:05 Processed 400 out of 476\n"
     ]
    }
   ],
   "source": [
    "id_persona_power_dict, \\\n",
    "    id_persona_total_dict, \\\n",
    "    id_nsubj_verb_count_dict, \\\n",
    "    id_dobj_verb_count_dict = pf.measure_power(verb_power_dict,\n",
    "                                               persona_pattern_dict, \n",
    "                                               stories, \n",
    "                                               story_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Examine the power scores\n",
    "\n",
    "Print the mean scores for each persona across the whole dataset.\n",
    "\n",
    "You'll need to decide how to combine the positive and negative power scores for each persona. Here, we'll subtract the negative score from the positive score and divide by the total number of entity mentions in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_persona_score_dict = defaultdict(lambda: defaultdict(float))\n",
    "for _id, _persona_power_dict in id_persona_power_dict.items():\n",
    "    for _persona, _polarity_score_dict in _persona_power_dict.items():\n",
    "        _score = _polarity_score_dict['positive'] - _polarity_score_dict['negative']\n",
    "        _score /= id_persona_total_dict[_id][_persona]\n",
    "        id_persona_score_dict[_id][_persona] = _score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_scores_dict = defaultdict(list)\n",
    "for _id, _persona_score_dict in id_persona_score_dict.items():\n",
    "    for _persona, _score in _persona_score_dict.items():\n",
    "        persona_scores_dict[_persona].append(_score)\n",
    "\n",
    "person_score_dict = {_persona: np.mean(_scores) for _persona, _scores in persona_scores_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masculine pronouns 0.0\n",
      "feminine pronouns 0.08\n",
      "neutral pronouns 0.28\n"
     ]
    }
   ],
   "source": [
    "for _persona, _score in person_score_dict.items():\n",
    "    print(_persona, round(_score, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Examine the verb coverage\n",
    "\n",
    "Print the verbs that were most frequently matched to the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "457 \t be\n",
      "230 \t have\n",
      "167 \t say\n",
      "163 \t go\n",
      "154 \t get\n",
      "122 \t tell\n",
      "119 \t come\n",
      "105 \t do\n",
      "100 \t start\n",
      "78 \t take\n",
      "76 \t know\n",
      "72 \t look\n",
      "69 \t call\n",
      "64 \t want\n",
      "61 \t ask\n",
      "59 \t see\n",
      "55 \t try\n",
      "52 \t think\n",
      "48 \t give\n",
      "44 \t leave\n"
     ]
    }
   ],
   "source": [
    "verb_count_dict = pf.evaluate_verb_coverage(id_nsubj_verb_count_dict)\n",
    "\n",
    "for _verb, _count in sorted(verb_count_dict.items(), key=lambda x: x[1], reverse=True)[:20]:\n",
    "    print(_count, '\\t', _verb) # + ' (' + str(_count) + ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>\n",
    "\n",
    "# Examine the persona coverage\n",
    "\n",
    "For each persona, print how often the persona was used with a verb that was matched or not matched to the lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_found_dict, persona_missed_dict, persona_total_dict = pf.evaluate_persona_coverage(id_persona_total_dict, id_persona_power_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "masculine pronouns\n",
      "Matched: 3492 (54.7%)\n",
      "Not Matched: 2888 (45.3%)\n",
      "\n",
      "feminine pronouns\n",
      "Matched: 1059 (53.2%)\n",
      "Not Matched: 931 (46.8%)\n",
      "\n",
      "neutral pronouns\n",
      "Matched: 377 (60.2%)\n",
      "Not Matched: 249 (39.8%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for _persona, _total in persona_total_dict.items():\n",
    "    print(_persona)\n",
    "    print('Matched:', persona_found_dict[_persona], '(' + str(round((persona_found_dict[_persona] / _total) * 100, 1)) + '%)')\n",
    "    print('Not Matched:', persona_missed_dict[_persona], '(' + str(round((persona_missed_dict[_persona] / _total) * 100, 1)) + '%)')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('python3.8env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "88435282e7afd386f137f5bf71124f6ca50142be149f9ca18724b5a580de610c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
