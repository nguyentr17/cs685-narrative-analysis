{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "nlp = spacy.load('en')\n",
    "\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(nlp)\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tagger', 'parser', 'ner', 'neuralcoref']\n"
     ]
    }
   ],
   "source": [
    "# print pipeline components\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "narratives = pd.read_csv('narratives.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = narratives['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed = nlp(texts[-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "agency_power_df = pd.read_csv('FramesAgencyPower/agency_power.csv')\n",
    "agency_power_verbs = agency_power_df['verb'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "verb = nlp('abolishes')#nlp(agency_power_verbs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abolishes abolishe\n"
     ]
    }
   ],
   "source": [
    "for sent in verb.sents:\n",
    "    #print(sent.text)\n",
    "    for token in sent:\n",
    "        print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abolish'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEMMATIZER.lemmatize('abolishes', pos='v')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = list(verb.sents)\n",
    "lemma = sents[0][0].lemma_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abandon'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For for\n",
      "context context\n",
      ", ,\n",
      "I -PRON-\n",
      "have have\n",
      "always always\n",
      "had have\n",
      "a a\n",
      "binge binge\n",
      "eating eat\n",
      "disorder disorder\n",
      ". .\n",
      "However however\n",
      ", ,\n",
      "during during\n",
      "covid covid\n",
      "it -PRON-\n",
      "hit hit\n",
      "a a\n",
      "huge huge\n",
      "spike spike\n",
      ". .\n",
      "I -PRON-\n",
      "gained gain\n",
      "40 40\n",
      "lbs lbs\n",
      "in in\n",
      "one one\n",
      "year year\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Growing grow\n",
      "up up\n",
      ", ,\n",
      "I -PRON-\n",
      "was be\n",
      "always always\n",
      "fat fat\n",
      "shamed shame\n",
      "even even\n",
      "though though\n",
      "at at\n",
      "the the\n",
      "time time\n",
      "I -PRON-\n",
      "was be\n",
      "120 120\n",
      "and and\n",
      "5’5 5’5\n",
      ". .\n",
      "As as\n",
      "you -PRON-\n",
      "can can\n",
      "guess guess\n",
      ", ,\n",
      "this this\n",
      "set set\n",
      "me -PRON-\n",
      "down down\n",
      "a a\n",
      "spiral spiral\n",
      "where where\n",
      "I -PRON-\n",
      "would would\n",
      "stress stress\n",
      "eat eat\n",
      "and and\n",
      "actually actually\n",
      "become become\n",
      "fat fat\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "I -PRON-\n",
      "would would\n",
      "always always\n",
      "make make\n",
      "excuses excuse\n",
      "to to\n",
      "not not\n",
      "workout workout\n",
      "or or\n",
      "to to\n",
      "not not\n",
      "binge binge\n",
      ". .\n",
      "I -PRON-\n",
      "was be\n",
      "miserable miserable\n",
      ". .\n",
      "It -PRON-\n",
      "was be\n",
      "obvious obvious\n",
      "that that\n",
      "people people\n",
      "had have\n",
      "notice notice\n",
      "my -PRON-\n",
      "weight weight\n",
      "gain gain\n",
      "and and\n",
      "they -PRON-\n",
      "all all\n",
      "started start\n",
      "treating treat\n",
      "me -PRON-\n",
      "quickly quickly\n",
      ". .\n",
      "My -PRON-\n",
      "senior senior\n",
      "year year\n",
      "of of\n",
      "high high\n",
      "school school\n",
      "was be\n",
      "hell hell\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "First first\n",
      "year year\n",
      "of of\n",
      "college college\n",
      ", ,\n",
      "I -PRON-\n",
      "was be\n",
      "away away\n",
      "from from\n",
      "home home\n",
      "and and\n",
      "started start\n",
      "to to\n",
      "feel feel\n",
      "independent independent\n",
      ". .\n",
      "Skip Skip\n",
      "forward forward\n",
      ", ,\n",
      "I -PRON-\n",
      "got get\n",
      "a a\n",
      "psychiatrist psychiatrist\n",
      "and and\n",
      "he -PRON-\n",
      "diagnosed diagnose\n",
      "me -PRON-\n",
      "with with\n",
      "BED BED\n",
      ". .\n",
      "It -PRON-\n",
      "was be\n",
      "nothing nothing\n",
      "new new\n",
      "really really\n",
      ". .\n",
      "However however\n",
      ", ,\n",
      "just just\n",
      "two two\n",
      "weeks week\n",
      "ago ago\n",
      ", ,\n",
      "he -PRON-\n",
      "prescribed prescribe\n",
      "me -PRON-\n",
      "vyvanse vyvanse\n",
      "and and\n",
      "it -PRON-\n",
      "so so\n",
      "far far\n",
      "has have\n",
      "significantly significantly\n",
      "changed change\n",
      "my -PRON-\n",
      "life life\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Vyvanse vyvanse\n",
      "is be\n",
      "not not\n",
      "a a\n",
      "weight weight\n",
      "loss loss\n",
      "drug drug\n",
      ", ,\n",
      "but but\n",
      "it -PRON-\n",
      "is be\n",
      "meant mean\n",
      "to to\n",
      "suppress suppress\n",
      "your -PRON-\n",
      "impulses impulse\n",
      "and and\n",
      "binge binge\n",
      "like like\n",
      "tendencies tendency\n",
      ". .\n",
      "Its -PRON-\n",
      "been be\n",
      "only only\n",
      "1.5 1.5\n",
      "week week\n",
      "and and\n",
      "I -PRON-\n",
      "have have\n",
      "already already\n",
      "loss los\n",
      "nearly nearly\n",
      "10 10\n",
      "pounds pound\n",
      "since since\n",
      "the the\n",
      "beginning beginning\n",
      "of of\n",
      "January January\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "However however\n",
      ", ,\n",
      "I -PRON-\n",
      "have have\n",
      "also also\n",
      "been be\n",
      "working work\n",
      "out out\n",
      ". .\n",
      "Nothing nothing\n",
      "too too\n",
      "intense intense\n",
      "really really\n",
      ", ,\n",
      "just just\n",
      "walking walk\n",
      "for for\n",
      "30 30\n",
      "minutes minute\n",
      "on on\n",
      "a a\n",
      "treadmill treadmill\n",
      "with with\n",
      "an an\n",
      "incline incline\n",
      ". .\n",
      "Vyvanse vyvanse\n",
      "is be\n",
      "also also\n",
      "prescribed prescribe\n",
      "for for\n",
      "patients patient\n",
      "with with\n",
      "ADHD ADHD\n",
      ", ,\n",
      "and and\n",
      "I -PRON-\n",
      "have have\n",
      "never never\n",
      "felt feel\n",
      "more more\n",
      "productive productive\n",
      "and and\n",
      "social social\n",
      "in in\n",
      "my -PRON-\n",
      "life life\n",
      ". .\n",
      "I -PRON-\n",
      "no no\n",
      "longer longer\n",
      "care care\n",
      "of of\n",
      "how how\n",
      "people people\n",
      "think think\n",
      "about about\n",
      "me -PRON-\n",
      "and and\n",
      "because because\n",
      "I -PRON-\n",
      "have have\n",
      "been be\n",
      "keeping keep\n",
      "myself -PRON-\n",
      "busy busy\n",
      ", ,\n",
      "I -PRON-\n",
      "have have\n",
      "n’t not\n",
      "thought think\n",
      "about about\n",
      "food food\n",
      "as as\n",
      "much much\n",
      "as as\n",
      "I -PRON-\n",
      "usually usually\n",
      "do do\n",
      ". .\n",
      "In in\n",
      "my -PRON-\n",
      "experience experience\n",
      ", ,\n",
      "its -PRON-\n",
      "also also\n",
      "kind kind\n",
      "of of\n",
      "like like\n",
      "an an\n",
      "appetite appetite\n",
      "suppressant suppressant\n",
      "but but\n",
      "do do\n",
      "n’t not\n",
      "quote quote\n",
      "me -PRON-\n",
      "on on\n",
      "that that\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Vyvanse vyvanse\n",
      "is be\n",
      "very very\n",
      "expensive expensive\n",
      ", ,\n",
      "and and\n",
      "I -PRON-\n",
      "just just\n",
      "feel feel\n",
      "so so\n",
      "fortunate fortunate\n",
      "that that\n",
      "my -PRON-\n",
      "insurance insurance\n",
      "covers cover\n",
      "it -PRON-\n",
      ". .\n",
      "Although although\n",
      "the the\n",
      "weight weight\n",
      "loss loss\n",
      "and and\n",
      "change change\n",
      "is be\n",
      "minimal minimal\n",
      ", ,\n",
      "I -PRON-\n",
      "already already\n",
      "fell fall\n",
      "so so\n",
      "much much\n",
      "better- better-\n",
      "like like\n",
      "there there\n",
      "is be\n",
      "hope hope\n",
      "for for\n",
      "me -PRON-\n",
      ". .\n",
      "\n",
      "\n",
      " \n",
      "\n",
      "\n",
      "Just just\n",
      "a a\n",
      "little little\n",
      "rant rant\n",
      ":) :)\n"
     ]
    }
   ],
   "source": [
    "for parsed_sent in parsed.sents:\n",
    "    for token in parsed_sent:\n",
    "        print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For context, I have always had a binge eating disorder.\n",
      "  context pobj\n",
      "    VERB: For\n",
      "  I nsubj\n",
      "    VERB: had\n",
      "  a binge dobj\n",
      "    VERB: had\n",
      "  disorder dobj\n",
      "    VERB: eating\n",
      "However, during covid it hit a huge spike.\n",
      "  covid pcomp\n",
      "    VERB: during\n",
      "  it nsubj\n",
      "    VERB: hit\n",
      "  a huge spike dobj\n",
      "    VERB: hit\n",
      "I gained 40 lbs in one year. \n",
      "\n",
      "\n",
      "  I nsubj\n",
      "    VERB: gained\n",
      "  40 lbs dobj\n",
      "    VERB: gained\n",
      "  one year pobj\n",
      "    VERB: in\n",
      "Growing up, I was always fat shamed even though at the time I was 120 and 5’5.\n",
      "  I nsubj\n",
      "    VERB: was\n",
      "  the time pobj\n",
      "    VERB: at\n",
      "  I nsubj\n",
      "    VERB: was\n",
      "As you can guess, this set me down a spiral where I would stress eat and actually become fat. \n",
      "\n",
      "\n",
      "  you nsubj\n",
      "    VERB: guess\n",
      "  me dobj\n",
      "    VERB: set\n",
      "  a spiral pobj\n",
      "    VERB: down\n",
      "  I nsubj\n",
      "    VERB: stress\n",
      "I would always make excuses to not workout or to not binge.\n",
      "  I nsubj\n",
      "    VERB: make\n",
      "  excuses dobj\n",
      "    VERB: make\n",
      "I was miserable.\n",
      "  I nsubj\n",
      "    VERB: was\n",
      "It was obvious that people had notice my weight gain\n",
      "  It nsubj\n",
      "    VERB: was\n",
      "  people nsubj\n",
      "    VERB: notice\n",
      "  my weight gain dobj\n",
      "    VERB: notice\n",
      "and they all started treating me quickly.\n",
      "  they nsubj\n",
      "    VERB: started\n",
      "  me dobj\n",
      "    VERB: treating\n",
      "My senior year of high school was hell. \n",
      "\n",
      "\n",
      "  My senior year nsubj\n",
      "    VERB: was\n",
      "  high school pobj\n",
      "    VERB: of\n",
      "  hell attr\n",
      "    VERB: was\n",
      "First year of college, I was away from home and started to feel independent.\n",
      "  college pobj\n",
      "    VERB: of\n",
      "  I nsubj\n",
      "    VERB: was\n",
      "  home pobj\n",
      "    VERB: from\n",
      "Skip forward\n",
      "  Skip ROOT\n",
      "    VERB: Skip\n",
      ", I got a psychiatrist and he diagnosed me with BED.\n",
      "  I nsubj\n",
      "    VERB: got\n",
      "  a psychiatrist dobj\n",
      "    COREF: a psychiatrist , a psychiatrist , got\n",
      "    VERB: got\n",
      "  he nsubj\n",
      "    COREF: he , a psychiatrist , diagnosed\n",
      "    VERB: diagnosed\n",
      "  me dobj\n",
      "    VERB: diagnosed\n",
      "  BED pobj\n",
      "    VERB: with\n",
      "It was nothing new really.\n",
      "  It nsubj\n",
      "    VERB: was\n",
      "  nothing attr\n",
      "    VERB: was\n",
      "However, just two weeks ago, he prescribed me vyvanse and it so far has significantly changed my life. \n",
      "\n",
      "\n",
      "  he nsubj\n",
      "    COREF: he , a psychiatrist , prescribed\n",
      "    VERB: prescribed\n",
      "  me dative\n",
      "    VERB: prescribed\n",
      "  vyvanse dobj\n",
      "    COREF: vyvanse , vyvanse , prescribed\n",
      "    VERB: prescribed\n",
      "  it nsubj\n",
      "    COREF: it , vyvanse , changed\n",
      "    VERB: changed\n",
      "  my life dobj\n",
      "    COREF: my life , my life , changed\n",
      "    VERB: changed\n",
      "Vyvanse is not a weight loss drug, but it is meant to suppress your impulses and binge like tendencies.\n",
      "  Vyvanse nsubj\n",
      "    COREF: Vyvanse , Vyvanse , is\n",
      "    VERB: is\n",
      "  a weight loss drug attr\n",
      "    VERB: is\n",
      "  it nsubjpass\n",
      "    COREF: it , Vyvanse , meant\n",
      "    VERB: meant\n",
      "  your impulses dobj\n",
      "    VERB: suppress\n",
      "  tendencies pobj\n",
      "    VERB: like\n",
      "Its been only 1.5 week and I have already loss nearly 10 pounds since the beginning of January. \n",
      "\n",
      "\n",
      "  I nsubj\n",
      "    VERB: loss\n",
      "  nearly 10 pounds dobj\n",
      "    VERB: loss\n",
      "  the beginning pobj\n",
      "    VERB: since\n",
      "  January pobj\n",
      "    VERB: of\n",
      "However, I have also been working out.\n",
      "  I nsubj\n",
      "    VERB: working\n",
      "Nothing too intense really, just walking for 30 minutes on a treadmill with an incline.\n",
      "  Nothing nsubj\n",
      "    VERB: walking\n",
      "  30 minutes pobj\n",
      "    VERB: for\n",
      "  a treadmill pobj\n",
      "    VERB: on\n",
      "  an incline pobj\n",
      "    VERB: with\n",
      "Vyvanse is also prescribed for patients with ADHD, and I have never felt more productive and social in my life.\n",
      "  Vyvanse nsubjpass\n",
      "    VERB: prescribed\n",
      "  patients pobj\n",
      "    VERB: for\n",
      "  ADHD pobj\n",
      "    VERB: with\n",
      "  I nsubj\n",
      "    VERB: felt\n",
      "  my life pobj\n",
      "    COREF: my life , my life , in\n",
      "    VERB: in\n",
      "I no longer care of how people think about me and because I have been keeping myself busy, I haven’t thought about food as much as I usually do.\n",
      "  I nsubj\n",
      "    VERB: care\n",
      "  people nsubj\n",
      "    VERB: think\n",
      "  me pobj\n",
      "    COREF: me , me , about\n",
      "    VERB: about\n",
      "  I nsubj\n",
      "    VERB: keeping\n",
      "  myself dobj\n",
      "    COREF: myself , me , keeping\n",
      "    VERB: keeping\n",
      "  I nsubj\n",
      "    VERB: thought\n",
      "  food pobj\n",
      "    VERB: about\n",
      "  I nsubj\n",
      "    VERB: do\n",
      "In my experience, its also kind of like an appetite suppressant but don’t quote me on that.\n",
      "\n",
      "\n",
      "  my experience pobj\n",
      "    VERB: In\n",
      "  an appetite suppressant pobj\n",
      "    VERB: like\n",
      "  me dobj\n",
      "    VERB: quote\n",
      "Vyvanse is very expensive, and I just feel so fortunate that my insurance covers it.\n",
      "  Vyvanse nsubj\n",
      "    COREF: Vyvanse , Vyvanse , is\n",
      "    VERB: is\n",
      "  I nsubj\n",
      "    VERB: feel\n",
      "  my insurance nsubj\n",
      "    VERB: covers\n",
      "  it dobj\n",
      "    COREF: it , Vyvanse , covers\n",
      "    VERB: covers\n",
      "Although the weight loss and change is minimal, I already fell so much better- like there is hope for me. \n",
      "\n",
      "\n",
      "  the weight loss nsubj\n",
      "    VERB: is\n",
      "  change conj\n",
      "    VERB: loss\n",
      "  I nsubj\n",
      "    VERB: fell\n",
      "  hope attr\n",
      "    VERB: is\n",
      "  me pobj\n",
      "    COREF: me , me , for\n",
      "    VERB: for\n",
      "Just a little rant :)\n",
      "  Just a little rant ROOT\n",
      "    VERB: rant\n"
     ]
    }
   ],
   "source": [
    "for parsed_sent in parsed.sents:\n",
    "    print(parsed_sent.text)\n",
    "    for noun_chunk in parsed_sent.noun_chunks:\n",
    "        print(' ', noun_chunk.text, noun_chunk.root.dep_)\n",
    "        '''\n",
    "        _verb = noun_chunk.root.head.lemma_.lower()\n",
    "        print(noun_chunk.root.head.text)\n",
    "        print('  Spacy lemma: ', _verb)\n",
    "        print('  WordNet lemma: ', LEMMATIZER.lemmatize(_verb, pos='v').lower())\n",
    "        '''\n",
    "        if noun_chunk._.is_coref:\n",
    "            print('    COREF:', noun_chunk.text, ',', noun_chunk._.coref_cluster.main.text, ',', noun_chunk.root.head.text)\n",
    "        \n",
    "        print('    VERB:', noun_chunk.root.head.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[my mom: [my mom, My mom], mom and dad: [mom and dad, them]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed._.coref_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed._.coref_resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed._.has_coref"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5eb4d484db7af08329a00b87bd1e951cd900c6ec4b73b24380553000cd34ae58"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
