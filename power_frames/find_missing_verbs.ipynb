{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse all reddit data; save resulting parsed spaCy docs; save set of all verb lemmas used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.lang.en.English at 0x15b8ce3d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import spacy\n",
    "NLP = spacy.load('en')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "LEMMATIZER = WordNetLemmatizer()\n",
    "\n",
    "import neuralcoref\n",
    "neuralcoref.add_to_pipe(NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner', 'neuralcoref']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NLP.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import DocBin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '../narrative_detection/narrative_posts_by_trained_classification.csv'\n",
    "dataset_df = pd.read_csv(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stories = dataset_df['selftext'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_stories = {i: NLP(story) for i, story in enumerate(stories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_list = [parsed_stories[i] for i in parsed_stories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parsed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bin = DocBin()\n",
    "for doc in parsed_list:\n",
    "    doc_bin.add(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_bytes = doc_bin.to_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/parsed_stories.pickle', 'wb') as f:\n",
    "    pickle.dump(doc_bytes, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_bytes = pickle.load(open('data/parsed_stories.pickle', 'rb'))\n",
    "saved_docs = DocBin().from_bytes(saved_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_docs = saved_docs.get_docs(NLP.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovered_docs = [doc for doc in recovered_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m recovered_docs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msents:\n\u001b[1;32m      2\u001b[0m     \u001b[39mfor\u001b[39;00m noun_chunk \u001b[39min\u001b[39;00m sent\u001b[39m.\u001b[39mnoun_chunks:\n\u001b[1;32m      3\u001b[0m         \u001b[39mprint\u001b[39m(noun_chunk\u001b[39m.\u001b[39mtext)\n",
      "File \u001b[0;32m~/miniconda3/envs/coref/lib/python3.11/site-packages/spacy/tokens/doc.pyx:652\u001b[0m, in \u001b[0;36msents\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E030] Sentence boundaries unset. You can add the 'sentencizer' component to the pipeline with: nlp.add_pipe(nlp.create_pipe('sentencizer')) Alternatively, add the dependency parser, or set sentence boundaries by setting doc[i].is_sent_start."
     ]
    }
   ],
   "source": [
    "for sent in recovered_docs[0].sents:\n",
    "    for noun_chunk in sent.noun_chunks:\n",
    "        print(noun_chunk.text)\n",
    "        print(noun_chunk._.is_coref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "[E112] Pickling a span is not supported, because spans are only views of the parent Doc and can't exist on their own. A pickled span would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the span, pickle the Doc it belongs to or use Span.as_doc to convert the span to a standalone Doc object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m [pickle\u001b[39m.\u001b[39mdumps(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m parsed_list]\n",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[39m=\u001b[39m [pickle\u001b[39m.\u001b[39mdumps(doc) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m parsed_list]\n",
      "File \u001b[0;32m~/miniconda3/envs/coref/lib/python3.11/site-packages/spacy/tokens/doc.pyx:1371\u001b[0m, in \u001b[0;36mspacy.tokens.doc.pickle_doc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/coref/lib/python3.11/site-packages/srsly/_pickle_api.py:14\u001b[0m, in \u001b[0;36mpickle_dumps\u001b[0;34m(data, protocol)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpickle_dumps\u001b[39m(data, protocol\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      8\u001b[0m     \u001b[39m\"\"\"Serialize a Python object with pickle.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \n\u001b[1;32m     10\u001b[0m \u001b[39m    data: The object to serialize.\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m    protocol (int): Protocol to use. -1 for highest.\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    RETURNS (bytest): The serialized object.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m     \u001b[39mreturn\u001b[39;00m cloudpickle\u001b[39m.\u001b[39mdumps(data, protocol\u001b[39m=\u001b[39mprotocol)\n",
      "File \u001b[0;32m~/miniconda3/envs/coref/lib/python3.11/site-packages/srsly/cloudpickle/cloudpickle_fast.py:73\u001b[0m, in \u001b[0;36mdumps\u001b[0;34m(obj, protocol, buffer_callback)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mwith\u001b[39;00m io\u001b[39m.\u001b[39mBytesIO() \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m     70\u001b[0m     cp \u001b[39m=\u001b[39m CloudPickler(\n\u001b[1;32m     71\u001b[0m         file, protocol\u001b[39m=\u001b[39mprotocol, buffer_callback\u001b[39m=\u001b[39mbuffer_callback\n\u001b[1;32m     72\u001b[0m     )\n\u001b[0;32m---> 73\u001b[0m     cp\u001b[39m.\u001b[39mdump(obj)\n\u001b[1;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m file\u001b[39m.\u001b[39mgetvalue()\n",
      "File \u001b[0;32m~/miniconda3/envs/coref/lib/python3.11/site-packages/srsly/cloudpickle/cloudpickle_fast.py:632\u001b[0m, in \u001b[0;36mCloudPickler.dump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdump\u001b[39m(\u001b[39mself\u001b[39m, obj):\n\u001b[1;32m    631\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 632\u001b[0m         \u001b[39mreturn\u001b[39;00m Pickler\u001b[39m.\u001b[39mdump(\u001b[39mself\u001b[39m, obj)\n\u001b[1;32m    633\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    634\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mrecursion\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m e\u001b[39m.\u001b[39margs[\u001b[39m0\u001b[39m]:\n",
      "File \u001b[0;32m~/miniconda3/envs/coref/lib/python3.11/site-packages/spacy/tokens/span.pyx:204\u001b[0m, in \u001b[0;36mspacy.tokens.span.Span.__reduce__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: [E112] Pickling a span is not supported, because spans are only views of the parent Doc and can't exist on their own. A pickled span would always have to include its Doc and Vocab, which has practically no advantage over pickling the parent Doc directly. So instead of pickling the span, pickle the Doc it belongs to or use Span.as_doc to convert the span to a standalone Doc object."
     ]
    }
   ],
   "source": [
    "data = [pickle.dumps(doc) for doc in parsed_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'_io.BufferedWriter' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# save parsed stories to file \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pickle\u001b[39m.\u001b[39mdumps(parsed_list, \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparsed_stories.pkl\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[0;31mTypeError\u001b[0m: '_io.BufferedWriter' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "# save parsed stories to file \n",
    "pickle.dumps(parsed_list, open('parsed_stories.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_verbs = set()\n",
    "for story in parsed_stories.values():\n",
    "    for sent in story.sents:\n",
    "        for noun_chunk in sent.noun_chunks:  \n",
    "            if noun_chunk.root.dep_ in ['nsubj', 'dobj']:\n",
    "                verb = noun_chunk.root.head.text\n",
    "                verb_lemma = LEMMATIZER.lemmatize(verb, pos='v').lower()\n",
    "                all_verbs.add(verb_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_path = 'FramesAgencyPower/agency_power.csv'\n",
    "lexicon_df = pd.read_csv(lexicon_path)\n",
    "\n",
    "# lemmatize verbs in lexicon\n",
    "lexicon_df['lemma'] = lexicon_df['verb'].apply(lambda x: LEMMATIZER.lemmatize(x, pos='v').lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexicon_verbs = set(lexicon_df['lemma'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/lexicon_verbs_lemmatized.txt', 'w') as f:\n",
    "    for verb in lexicon_verbs:\n",
    "        f.write(verb + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verbs missing from lexicon\n",
    "missing_verbs = all_verbs - lexicon_verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save missing verbs to file\n",
    "with open('missing_verbs.txt', 'w') as f:\n",
    "    for verb in missing_verbs:\n",
    "        f.write(verb + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/missing_verbs.txt', 'r') as f:\n",
    "    missing_verbs = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2625"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(missing_verbs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coref",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
